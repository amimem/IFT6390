{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme:\n",
    "\n",
    "Phase 1:\n",
    "NLTK stop words are used and extended based on the resulting bag of words.\n",
    "\n",
    "In the train fucntion the test set is cleaned using scikit stopwords, ( expressions are taken from some web Medium articles ) and the probablity of each class and the count matrix of each word for each class and a mapping from word to index is returnd.\n",
    "\n",
    "calculate_prob takes these outputs and a test set, cleans it and for each comment calculates the log of p(comment|class) using laplace smoothing.\n",
    "\n",
    "redults for different alphas are compared on a validation set.\n",
    "\n",
    "the final results are saved using pandas\n",
    "\n",
    "Phase 2:\n",
    "To run other methods just import necessary libraries and run the cells.\n",
    "numpy, scikit, nltk, pandas, keras and fastai are used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/iDev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/iDev/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/iDev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, implementation from scratch:\n",
    "\n",
    "List of stopwords in Scikit, NLTK and a handmade extension\n",
    "Text cleaning methods sources:\n",
    "\n",
    "\n",
    "https://sdsawtelle.github.io/blog/output/spam-classification-part2-vectorization-and-svm-pipeline.html\n",
    "\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/stemming-lemmatization-python\n",
    "\n",
    "\n",
    "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_scikit = ['now', 'fill', 'onto', 'get', 'whereupon', 'too', 'if', 'everything', 'etc', 'almost', 'which', 'yet', 'either', 'that', 'someone', 'wherein', 'sixty', 'empty', 'anyway', 'beside', 'other', 'full', 'amoungst', 'same', 'least', 'namely', 'system', 'two', 'hundred', 'ten', 'became', 'please', 'our', 'formerly', 'twenty', 'go', 'to', 'each', 'afterwards', 'twelve', 'call', 'all', 'five', 'none', 'been', 'ever', 'much', 'can', 'such', 'since', 'enough', 'again', 'ie', 'him', 'being', 'hers', 'well', 'what', 'might', 'keep', 'these', 'when', 'whose', 'meanwhile', 'yourselves', 'under', 'nobody', 'for', 'or', 'own', 'cry', 'nothing', 'until', 'anything', 'me', 'but', 'together', 'where', 'most', 'whence', 'after', 'ours', 'last', 'anyone', 'thru', 'whole', 'often', 'thick', 'very', 'with', 'further', 'wherever', 'although', 'she', 'above', 'upon', 'somewhere', 'once', 'them', 'off', 'even', 'who', 'bill', 'eg', 'fifteen', 'must', 'us', 'seeming', 'found', 'how', 'next', 'seems', 'noone', 'would', 'hasnt', 'thin', 'take', 'part', 'they', 'out', 'within', 'interest', 'his', 'toward', 'then', 'never', 'am', 'every', 'throughout', 'always', 'itself', 'against', 'up', 'move', 'along', 'an', 'via', 'beforehand', 'were', 'there', 'amongst', 'seem', 'about', 'few', 'yours', 'at', 'amount', 'herself', 'should', 'mostly', 'sometimes', 'was', 'from', 'moreover', 'thereby', 'he', 'co', 'forty', 'still', 'already', 'whereafter', 'the', 'hence', 'made', 'i', 'see', 'fifty', 'mill', 'something', 'among', 'themselves', 'her', 'everyone', 'one', 'before', 'your', 'why', 'towards', 'eight', 'it', 'cant', 'more', 'as', 'latter', 'find', 'whither', 'latterly', 'back', 'put', 'many', 'hereby', 'thereafter', 'between', 'else', 'here', 'inc', 'describe', 'becoming', 'nowhere', 'both', 'whoever', 'over', 'eleven', 'nor', 'becomes', 'across', 'anyhow', 'besides', 'in', 'whenever', 'though', 'have', 'name', 'no', 'elsewhere', 'do', 'nine', 'whether', 'serious', 'and', 'sometime', 'done', 'this', 'during', 'per', 'cannot', 'third', 'de', 'may', 'otherwise', 'whereas', 'however', 'behind', 'whom', 'only', 'due', 'are', 'down', 'hereafter', 'former', 'six', 'through', 'nevertheless', 'is', 'himself', 'on', 'has', 'mine', 'a', 'hereupon', 'than', 'thence', 'top', 'perhaps', 'its', 'four', 'somehow', 'less', 'seemed', 'couldnt', 'con', 'neither', 'several', 'herein', 'therein', 'bottom', 'by', 'while', 'my', 'those', 'also', 'everywhere', 'ltd', 'around', 'side', 'we', 'first', 'sincere', 'beyond', 'give', 'others', 'un', 'thus', 'another', 'detail', 'ourselves', 'their', 'below', 'be', 'any', 'myself', 'had', 're', 'thereupon', 'could', 'front', 'you', 'therefore', 'fire', 'whatever', 'indeed', 'rather', 'without', 'whereby', 'because', 'some', 'anywhere', 'show', 'alone', 'of', 'so', 'except', 'will', 'become', 'three', 'not', 'yourself', 'into']\n",
    "stop_nltk = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "extend = ['the', 'httpaddr', 'emailaddr', 'questmark', 'number', 'dollar', 'exclammark', 'ha','http','com','www','fucking','like', 'wa', 'one', 'would', 'get', 'people', 'think', 'nt', 'just', 'peopl', 'becaus', 'time','make','gt', 'did']\n",
    "\n",
    "punc = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text, label, classes, stopwords):\n",
    "    \n",
    "    start = time.process_time()\n",
    "    word_index = {}\n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    listed_text = text.tolist()\n",
    "    index = 0\n",
    "    for i, comment in enumerate(listed_text):\n",
    "        \n",
    "       # Converting to Lowercase\n",
    "        comment = comment.lower()\n",
    "\n",
    "        tokenized = word_tokenize(comment)\n",
    "        \n",
    "        stemmed = [porter.stem(word) for word in tokenized]\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in stemmed]\n",
    "        cleaned = [word for word in stripped if word.isalpha()]\n",
    "        comment = [word for word in cleaned if not word in stopwords]\n",
    "        listed_text[i] = ' '.join(comment)\n",
    "                       \n",
    "        for token in listed_text[i].split():\n",
    "            if token not in word_index.keys():\n",
    "                word_index[token] = index\n",
    "                index += 1\n",
    "                \n",
    "    print(\"total words: \",len(word_index))\n",
    "\n",
    "    p_class = np.zeros(classes.shape[0])\n",
    "    \n",
    "    text = np.array(listed_text)\n",
    "    \n",
    "    matrix = np.zeros((len(word_index), classes.shape[0]))\n",
    "    \n",
    "    for i, c in enumerate(classes):\n",
    "        comments = text[label == c]\n",
    "        p_class[i] = comments.shape[0] / label.shape[0]\n",
    "        \n",
    "        for comment in comments:\n",
    "            for word in comment.split():\n",
    "                row = word_index[word]\n",
    "                matrix[row,i] += 1\n",
    "                \n",
    "    end = time.process_time()\n",
    "\n",
    "    print(\"training:\", end - start)\n",
    "    return (word_index, p_class, matrix, listed_text)\n",
    "\n",
    "def calculate_prob(text, word_index, count_matrix, p_class, classes, stopwords, alpha=1):\n",
    "    \n",
    "    start = time.process_time()\n",
    "    results = np.zeros(text.shape[0])\n",
    "    count_matrix = np.add(count_matrix,alpha)\n",
    "    division_vector = count_matrix.sum(axis = 0) + alpha*count_matrix.shape[0]\n",
    "    probability_mat = np.divide(count_matrix,division_vector)\n",
    "    log_prob_mat = np.log(probability_mat)\n",
    "    \n",
    "    class_index = np.arange(classes.shape[0])\n",
    "    \n",
    "    d = zip(class_index,classes)\n",
    "    \n",
    "    index_class = dict(d)\n",
    "    \n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    \n",
    "    ltext = text.tolist()\n",
    "    \n",
    "    for i, comment in enumerate(ltext):\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        comment = comment.lower()\n",
    "        \n",
    "        tokenized = word_tokenize(comment)\n",
    "        \n",
    "        stemmed = [porter.stem(word) for word in tokenized]\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in stemmed]\n",
    "        cleaned = [word for word in stripped if word.isalpha()]\n",
    "        comment = [word for word in cleaned if not word in stopwords]\n",
    "        ltext[i] = ' '.join(comment)\n",
    "    \n",
    "        vec = np.zeros(count_matrix.shape[1])\n",
    "        \n",
    "        for word in ltext[i].split():\n",
    "            if word in word_index.keys():\n",
    "                row = word_index[word]\n",
    "                vec += log_prob_mat[row]\n",
    "            else:\n",
    "                vec += np.divide(np.ones(count_matrix.shape[1])*alpha,division_vector)\n",
    "                \n",
    "        vec += np.log(p_class)\n",
    "        results[i] = np.argmax(vec)\n",
    "        \n",
    "    end = time.process_time()\n",
    "\n",
    "    print(\"pred :\", end - start)             \n",
    "    return (results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(3395)\n",
    "\n",
    "dataset_text , dataset_label = np.load('data_train-1.pkl',allow_pickle=True)\n",
    "test_text = np.load('data_test-2.pkl',allow_pickle=True)\n",
    "\n",
    "dataset_text = np.array(dataset_text)\n",
    "dataset_label = np.array(dataset_label)\n",
    "\n",
    "\n",
    "(train_text, validation_text, \n",
    " train_label, validation_label) = train_test_split(dataset_text, dataset_label, test_size=0.1 , random_state=3395)\n",
    "\n",
    "\n",
    "classes = np.unique(dataset_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words:  52005\n",
      "training: 62.569160999999895\n"
     ]
    }
   ],
   "source": [
    "word_index, p_class, count_matrix, cleaned_text = train(train_text, train_label, classes, set(stop_scikit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred : 7.013396000000284\n",
      "Acc :\t 0.6 \t 55.17142857142857\n"
     ]
    }
   ],
   "source": [
    "pred_ind = calculate_prob(validation_text, word_index, count_matrix, p_class, classes, set(stop_scikit), alpha=0.12)\n",
    "\n",
    "pred_labels = classes[pred_ind.astype(int)]\n",
    "\n",
    "print(\"Acc :\\t\" , a, \"\\t\", (np.sum(validation_label == pred_labels) / pred_labels.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "start = time.process_time()\n",
    "word_index, p_class, count_matrix = train(dataset_text, dataset_label, classes, set(stop_scikit))\n",
    "pred_ind = calculate_prob(np.array(test_text), word_index, count_matrix, p_class, classes, set(stop_scikit), alpha=0.1)\n",
    "pred_labels = classes[pred_ind.astype(int)]\n",
    "pd.DataFrame(pred_labels, columns=['Category']).to_csv(\"submit_naive_bayes_smoothing_ll.csv\",header=True,index=True,index_label=\"Id\")\n",
    "end = time.process_time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes using Scikit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_scikit = ['now', 'fill', 'onto', 'get', 'whereupon', 'too', 'if', 'everything', 'etc', 'almost', 'which', 'yet', 'either', 'that', 'someone', 'wherein', 'sixty', 'empty', 'anyway', 'beside', 'other', 'full', 'amoungst', 'same', 'least', 'namely', 'system', 'two', 'hundred', 'ten', 'became', 'please', 'our', 'formerly', 'twenty', 'go', 'to', 'each', 'afterwards', 'twelve', 'call', 'all', 'five', 'none', 'been', 'ever', 'much', 'can', 'such', 'since', 'enough', 'again', 'ie', 'him', 'being', 'hers', 'well', 'what', 'might', 'keep', 'these', 'when', 'whose', 'meanwhile', 'yourselves', 'under', 'nobody', 'for', 'or', 'own', 'cry', 'nothing', 'until', 'anything', 'me', 'but', 'together', 'where', 'most', 'whence', 'after', 'ours', 'last', 'anyone', 'thru', 'whole', 'often', 'thick', 'very', 'with', 'further', 'wherever', 'although', 'she', 'above', 'upon', 'somewhere', 'once', 'them', 'off', 'even', 'who', 'bill', 'eg', 'fifteen', 'must', 'us', 'seeming', 'found', 'how', 'next', 'seems', 'noone', 'would', 'hasnt', 'thin', 'take', 'part', 'they', 'out', 'within', 'interest', 'his', 'toward', 'then', 'never', 'am', 'every', 'throughout', 'always', 'itself', 'against', 'up', 'move', 'along', 'an', 'via', 'beforehand', 'were', 'there', 'amongst', 'seem', 'about', 'few', 'yours', 'at', 'amount', 'herself', 'should', 'mostly', 'sometimes', 'was', 'from', 'moreover', 'thereby', 'he', 'co', 'forty', 'still', 'already', 'whereafter', 'the', 'hence', 'made', 'i', 'see', 'fifty', 'mill', 'something', 'among', 'themselves', 'her', 'everyone', 'one', 'before', 'your', 'why', 'towards', 'eight', 'it', 'cant', 'more', 'as', 'latter', 'find', 'whither', 'latterly', 'back', 'put', 'many', 'hereby', 'thereafter', 'between', 'else', 'here', 'inc', 'describe', 'becoming', 'nowhere', 'both', 'whoever', 'over', 'eleven', 'nor', 'becomes', 'across', 'anyhow', 'besides', 'in', 'whenever', 'though', 'have', 'name', 'no', 'elsewhere', 'do', 'nine', 'whether', 'serious', 'and', 'sometime', 'done', 'this', 'during', 'per', 'cannot', 'third', 'de', 'may', 'otherwise', 'whereas', 'however', 'behind', 'whom', 'only', 'due', 'are', 'down', 'hereafter', 'former', 'six', 'through', 'nevertheless', 'is', 'himself', 'on', 'has', 'mine', 'a', 'hereupon', 'than', 'thence', 'top', 'perhaps', 'its', 'four', 'somehow', 'less', 'seemed', 'couldnt', 'con', 'neither', 'several', 'herein', 'therein', 'bottom', 'by', 'while', 'my', 'those', 'also', 'everywhere', 'ltd', 'around', 'side', 'we', 'first', 'sincere', 'beyond', 'give', 'others', 'un', 'thus', 'another', 'detail', 'ourselves', 'their', 'below', 'be', 'any', 'myself', 'had', 're', 'thereupon', 'could', 'front', 'you', 'therefore', 'fire', 'whatever', 'indeed', 'rather', 'without', 'whereby', 'because', 'some', 'anywhere', 'show', 'alone', 'of', 'so', 'except', 'will', 'become', 'three', 'not', 'yourself', 'into']\n",
    "stop_nltk = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "extend = ['ha','http','com','www','fucking','like', 'wa', 'one', 'would', 'get', 'people', 'think', 'nt', 'just', 'peopl', 'becaus', 'time','make','gt', 'did']\n",
    "\n",
    "punc = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords):\n",
    "    start = time.process_time()\n",
    "    porter = SnowballStemmer(\"english\")\n",
    "    listed_text = text.tolist()\n",
    "    index = 0\n",
    "    for i, comment in enumerate(listed_text):\n",
    "        \n",
    "        # Converting to Lowercase\n",
    "        comment = comment.lower()\n",
    "\n",
    "        # Replace all URLs with special strings\n",
    "        regx = re.compile(r\"(http|https)://[^\\s]*\")\n",
    "        comment, nhttps = regx.subn(repl=\" httpaddr \", string=comment)\n",
    "\n",
    "        # Replace all email addresses with special strings\n",
    "        regx = re.compile(r\"\\b[^\\s]+@[^\\s]+[.][^\\s]+\\b\")\n",
    "        comment, nemails = regx.subn(repl=\" emailaddr \", string=comment)\n",
    "\n",
    "        # Replace all numbers with special strings\n",
    "        regx = re.compile(r\"\\b[\\d.]+\\b\")\n",
    "        comment = regx.sub(repl=\" number \", string=comment)\n",
    "\n",
    "        # Replace all $, ! and ? with special strings\n",
    "        regx = re.compile(r\"[$]\")\n",
    "        comment = regx.sub(repl=\" dollar \", string=comment)\n",
    "        regx = re.compile(r\"[!]\")\n",
    "        comment = regx.sub(repl=\" exclammark \", string=comment)\n",
    "        regx = re.compile(r\"[?]\")\n",
    "        comment = regx.sub(repl=\" questmark \", string=comment)\n",
    "\n",
    "        # Remove all other punctuation (replace with white space)\n",
    "        regx = re.compile(r\"([^\\w\\s]+)|([_-]+)\")  \n",
    "        comment = regx.sub(repl=\" \", string=comment)\n",
    "\n",
    "        \n",
    "        # Make all white space a single space\n",
    "        regx = re.compile(r\"\\s+\")\n",
    "        comment = regx.sub(repl=\" \", string=comment)\n",
    "        \n",
    "        tokenized = word_tokenize(comment)\n",
    "        \n",
    "        stemmed = [porter.stem(word) for word in tokenized]\n",
    "        \n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in stemmed]\n",
    "        cleaned = [word for word in stripped if word.isalpha()]\n",
    "        comment = [word for word in cleaned if not word in stopwords]\n",
    "        listed_text[i] = ' '.join(comment)\n",
    "                \n",
    "    end = time.process_time()\n",
    "\n",
    "    print(\"time:\", end - start)\n",
    "    return listed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data management\n",
    "dataset_text , dataset_label = np.load('data_train-1.pkl',allow_pickle=True)\n",
    "test_text = np.load('data_test-2.pkl',allow_pickle=True)\n",
    "\n",
    "dataset_text = np.array(dataset_text)\n",
    "dataset_label = np.array(dataset_label)\n",
    "\n",
    "test_text = np.array(test_text)\n",
    "\n",
    "(train_text, validation_text, \n",
    " train_label, validation_label) = train_test_split(dataset_text, dataset_label, test_size=0.1 , random_state=3395)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48.08176\n",
      "time: 5.409060999999994\n"
     ]
    }
   ],
   "source": [
    "cleaned_train_text = clean_text(train_text, set(stop_scikit))\n",
    "cleaned_validation_text = clean_text(validation_text, set(stop_scikit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 54.433991000000006\n",
      "time: 22.197136\n"
     ]
    }
   ],
   "source": [
    "cleaned_dataset_text = clean_text(dataset_text, set(stop_scikit))\n",
    "cleaned_test_text = clean_text(test_text, set(stop_scikit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB-Acc :\t [57.07142857 56.9        57.62857143 57.82857143 57.14285714 56.75714286\n",
      " 57.48571429 56.81428571 56.92857143 57.5       ]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer( ngram_range = (1,1) , max_df=1.0, min_df=2, max_features=None, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True )\n",
    "\n",
    "dataset_tf= tf.fit_transform(cleaned_dataset_text)\n",
    "\n",
    "classifier = MultinomialNB(alpha=0.3)\n",
    "\n",
    "score = cross_val_score(classifier, dataset_tf, dataset_label, cv=10)\n",
    "\n",
    "print(\"MNB-Acc :\\t\" ,\n",
    "      score * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC Using Scikit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM-Acc :\t [56.3        56.27142857 57.35714286 57.07142857 56.17142857 56.24285714\n",
      " 56.72857143 56.25714286 56.58571429 56.44285714]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range = (1,1) , max_df=1.0, min_df=2, max_features=None, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True )\n",
    "\n",
    "dataset_tf= tf.fit_transform(cleaned_dataset_text)\n",
    "\n",
    "classifier = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=0.3)\n",
    "\n",
    "score = cross_val_score(classifier, dataset_tf, dataset_label, cv=10)\n",
    "\n",
    "print(\"SVM-Acc :\\t\" ,\n",
    "score * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression Using Scikit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_enc = preprocessing.LabelEncoder()\n",
    "label_enc.fit(dataset_label)\n",
    "\n",
    "train_target = label_enc.transform(train_label) \n",
    "validation_target = label_enc.transform(validation_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iDev/miniconda2/envs/drm/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1544: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 8.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-Acc :\t [55.31428571 55.58571429 55.98571429 56.12857143 55.52857143 55.05714286\n",
      " 55.5        55.14285714 55.55714286 55.4       ]\n"
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(ngram_range = (1,1) , max_df=1.0, min_df=2, max_features=None, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True )\n",
    "\n",
    "dataset_tf= tf.fit_transform(cleaned_dataset_text)\n",
    "\n",
    "classifier = LogisticRegression(C=2.8, penalty = 'l2', solver = 'liblinear', class_weight = None , multi_class = 'auto', max_iter = 1000, n_jobs=-1)\n",
    "score = cross_val_score(classifier, dataset_tf, dataset_label, cv=10)\n",
    "print(\"LogReg-Acc :\\t\",\n",
    "      score * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting Classifier using Scikit:\n",
    "\n",
    "source:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "c0 = MultinomialNB(alpha=0.3)\n",
    "c1 = LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg-Acc :\t [56.41428571 56.45714286 57.17142857 57.21428571 56.45714286 56.38571429\n",
      " 57.1        56.14285714 56.74285714 56.98571429]\n"
     ]
    }
   ],
   "source": [
    "v0 = VotingClassifier(estimators=[('mnb', c0), ('svm', c1),], voting='hard')\n",
    "\n",
    "score = cross_val_score(v0, dataset_tf, dataset_label, cv=10)\n",
    "print(\"LogReg-Acc :\\t\",\n",
    "      score * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VotingClassifier(estimators=[('mnb', c0), ('svm', c1)], voting='soft')\n",
    "score = cross_val_score(v1, dataset_tf, dataset_label, cv=10)\n",
    "print(\"LogReg-Acc :\\t\",\n",
    "      score * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN Using Keras\n",
    "\n",
    "Source:\n",
    "\n",
    "https://github.com/kk7nc/Text_Classification#deep-neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  Dropout, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(ngram_range = (1,1) , max_df=1.0, min_df=2, max_features=None, norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True )\n",
    "\n",
    "train_tf= tf.fit_transform(cleaned_train_text)\n",
    "validation_tf= tf.transform(cleaned_validation_text)\n",
    "test_tf = tf.transform(cleaned_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "nodes = 512\n",
    "layers = 1\n",
    "input_dim = train_tf.shape[1]\n",
    "dropout_rate = 0.5\n",
    "class_num = 20\n",
    "\n",
    "model.add(Dense(nodes,input_dim=input_dim,activation='relu'))\n",
    "model.add(Dropout(dropout_rate))\n",
    "for i in range(0,layers):\n",
    "    model.add(Dense(nodes,input_dim=nodes,activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(class_num, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               10958336  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                10260     \n",
      "=================================================================\n",
      "Total params: 11,231,252\n",
      "Trainable params: 11,231,252\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63000 samples, validate on 7000 samples\n",
      "Epoch 1/1\n",
      " - 118s - loss: 1.2366 - acc: 0.6411 - val_loss: 1.4772 - val_acc: 0.5666\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_tf, train_target,\n",
    "                              validation_data=(validation_tf, validation_target),\n",
    "                              epochs=1,\n",
    "                              batch_size=128,\n",
    "                              verbose=2)\n",
    "\n",
    "predicted = model.predict(validation_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.25      0.27       355\n",
      "           1       0.64      0.68      0.66       377\n",
      "           2       0.65      0.72      0.69       349\n",
      "           3       0.80      0.64      0.71       348\n",
      "           4       0.60      0.63      0.61       345\n",
      "           5       0.65      0.65      0.65       341\n",
      "           6       0.41      0.52      0.46       333\n",
      "           7       0.45      0.40      0.43       346\n",
      "           8       0.47      0.59      0.52       367\n",
      "           9       0.29      0.18      0.22       383\n",
      "          10       0.63      0.74      0.68       337\n",
      "          11       0.75      0.56      0.65       353\n",
      "          12       0.66      0.71      0.68       352\n",
      "          13       0.49      0.59      0.54       352\n",
      "          14       0.67      0.61      0.64       329\n",
      "          15       0.60      0.65      0.62       362\n",
      "          16       0.66      0.66      0.66       346\n",
      "          17       0.53      0.56      0.54       372\n",
      "          18       0.41      0.32      0.36       330\n",
      "          19       0.67      0.70      0.68       323\n",
      "\n",
      "    accuracy                           0.57      7000\n",
      "   macro avg       0.57      0.57      0.56      7000\n",
      "weighted avg       0.56      0.57      0.56      7000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(validation_target, np.argmax(predicted, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pretrained models:\n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "\n",
    "dataset_df = pd.DataFrame({'label': dataset_label , 'text': cleaned_dataset_text2 })\n",
    "\n",
    "train_df , validation_df = train_test_split(dataset_df, stratify = dataset_df['label'], test_size = 0.2, random_state = 3395)\n",
    "\n",
    "lang_model_data = TextLMDataBunch.from_df(train_df = train_df, valid_df = validation_df, path = \"\")\n",
    "\n",
    "classifier_data = TextClasDataBunch.from_df(train_df = train_df, valid_df = validation_df, path = \"\", vocab=data_lm.train_ds.vocab, bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learner = language_model_learner(lang_model_data, AWD_LSTM, drop_mult = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 2.29E-02\n",
      "Min loss divided by 10: 2.51E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_learner.lr_find()\n",
    "model_learner.recorder.plot(suggestion=True)\n",
    "lr_min = model_learner.recorder.min_grad_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='176' class='' max='254', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      69.29% [176/254 16:45<07:25 7.6083]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_learner.fit_one_cycle(1, lr_min)\n",
    "model_learner.save_encoder('myEnc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learner = text_classifier_learner(classifier_data, AWD_LSTM, drop_mult=0.7)\n",
    "model_learner.load_encoder('myEnc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_learner.fit_one_cycle(1, lr_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, targets = model_learner.get_preds()\n",
    "\n",
    "predictions = np.argmax(predictions, axis = 1)\n",
    "pd.crosstab(predictions, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
